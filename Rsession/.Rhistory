plot(lda.fit)
plot(lda.fit)
Smarket.2005=subset(Smarket,Year==2005)
lda.pred=predict(lda.fit,Smarket.2005)
lda.pred[1:5,]
class(lda.pred)
lda.fit=lda(Direction~Lag1+Lag2,data=Smarket, subset=Year<2005)
lda.fit
plot(lda.fit)
Smarket.2005=subset(Smarket,Year==2005)
lda.pred=predict(lda.fit,Smarket.2005)
lda.pred[1:5,]
class(lda.pred)
data.frame(lda.pred)[1:5,]
table(lda.pred$class,Smarket.2005$Direction)
mean(lda.pred$class==Smarket.2005$Direction)
library(class)
library(class)
attach(Smarket)
Xlag=cbind(Lag1,Lag2)
View(Xlag)
Xlag[1:5,]
mean(knn.pred==Direction[!train])
train=Year<2005
knn.pred=knn(Xlag[train,],Xlag[!train,],Direction[train],k=1)
table(knn.pred,Direction[!train])
mean(knn.pred==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred2,Direction[!train])
# Here k = 1 is useless, 0.5 like a flipping a coin.
mean(knn.pred2==Direction[!train])
# Use another k = 5
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 5)
table(knn.pred3,Direction[!train])
# Here k = 3  improves performance, 0.5357 like a flipping a coin.
mean(knn.pred3==Direction[!train])
# Use another k = 5
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 4)
table(knn.pred3,Direction[!train])
# Here k = 3  improves performance, 0.5357 like a flipping a coin.
mean(knn.pred3==Direction[!train])
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred3,Direction[!train])
# Here k = 3  improves performance, 0.5357 like a flipping a coin.
mean(knn.pred3==Direction[!train])
# Use another k = 4
knn.pred4=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 4)
table(knn.pred4,Direction[!train])
# Here k = 4 reduces performance, 0.5357
mean(knn.pred4==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred=knn(Xlag[train,],Xlag[!train,],Direction[train],k=1)
table(knn.pred,Direction[!train])
# Here k = 1 is useless, 0.5 like a flipping a coin.
mean(knn.pred==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
require(ISLR)
require(boot)
?cv.glm
plot(mpg~horsepower,data=Auto)
glm.fit=glm(mpg~horsepower, data=Auto)
summary(glm.fit)
cv.glm(Auto,glm.fit)$delta #pretty slow (doesnt use formula (5.2) on page 180)
loocv=function(fit){
h=lm.influence(fit)$h
# Element by element devidion
mean((residuals(fit)/(1-h))^2)
}
loocv(glm.fit)
cv.error=rep(0,5)
cv.error=rep(0,5)
cv.error=rep(0,5)
degree=1:5
for(d in degree){
glm.fit=glm(mpg~poly(horsepower,d), data=Auto)
cv.error[d]=loocv(glm.fit)
}
plot(degree,cv.error,type="b")
glm.fit=glm(mpg~horsepower, data=Auto)
summary(glm.fit)
?cv.glm
summary(glm.fit)
?glm
summary(glm.fit)
?glm.fit
summary(glm.fit)
?cv.glm
alpha=function(x,y){
vx=var(x)
vy=var(y)
cxy=cov(x,y)
(vy-cxy)/(vx+vy-2*cxy)
}
alpha(Portfolio$X,Portfolio$Y)
alpha.fn=function(data, index){
with(data[index,],alpha(X,Y))
}
alpha.fn(Portfolio,1:100)
summary(Portfolio)
str(Portfolio)
set.seed(1)
alpha.fn (Portfolio,sample(1:100,100,replace=TRUE))
boot.out=boot(Portfolio,alpha.fn,R=1000)
boot.out
plot(boot.out)
library(JGR)
JGR()
data <- 10 * c(11)
data <- 10 * c(11)
print(data)
age11 <- rep(11,10)
age12 <- rep(12,9)
age13 <- rep(13,11)
age14 <- rep(14,14)
age15 <- rep(15,10)
age16 <- rep(16,6)
data <- c(age11,age12,age13,age14,age15,age16)
mean(data)
sd(data)
median(data)
IQR(data)
hist(data)
hist(data,binwidth =1)
?hist
hist(data,breaks= c(11,12,13,14,15,16))
hist(data,breaks= c(11,12,13,14,15,16,17))
hist(data,breaks= c(10,11,12,13,14,15,16,17))
hist(data,breaks= c(10,11,12,13,14,15,16))
library(ggplot2)
qplot(data)
question2Data <- c(rep(0,53),rep(1,47))
mean(question2Data)
sd(question2Data)
median(question2Data)
IQR(question2Data)
data <- c(age11,age12,age13,age14,age15,age16)
mean(data)
sd(data)
median(data)
print(data)
IQR(data)
question2Data <- c(rep(0,53),rep(1,47))
mean(question2Data)
sd(question2Data)
median(question2Data)
IQR(question2Data)
library(JGR)
JGR()
a.w=2
a.c =3
a
(0.29)^2
(0.29)^2 * 0.8 + (1-0.29)^2 * 0.13 + (2-0.29)^2 * 0.05 + (3-0.29)^2 * 0.2
(0-0.29)^2 * 0.8 + (1-0.29)^2 * 0.13 + (2-0.29)^2 * 0.05 + (3-0.29)^2 * 0.2
(0-0.29)^2 * 0.8 + (1-0.29)^2 * 0.13 + (2-0.29)^2 * 0.05 + (3-0.29)^2 * 0.02
sqrt((0-0.29)^2 * 0.8 + (1-0.29)^2 * 0.13 + (2-0.29)^2 * 0.05 + (3-0.29)^2 * 0.02)
0.5^5 * 5 * 100
(81-75)/sqrt(150*0.5^2)
sqrt(500*0.05*0.95)
library(JGR)
JGR()
aug2 <- read.csv("C:/Users/zhushun0008/Desktop/aug2.csv", header=F)
View(aug2)
install.packages("igraph")
require(igraph)
a<-read.graph('METHODSC.NET', 'pajek')
METHODSC <- read.table("F:/SkyDrive/Studying/Stanford/SocialAndEconomicNetworksModelsAnalysis/homework/PA01/METHODSC.NET", quote="\"")
View(METHODSC)
framingham = read.csv("F:/SkyDrive/Studying/MIT_COURSES/15-071-TheAnalyticsEdge/lecture/dataset/framingham.csv")
str(framingham)
library(caTools)
train = subset(framingham, split==TRUE)
test = subset(framingham, split==FALSE)
# Logistic Regression Model
framinghamLog = glm(TenYearCHD ~ ., data = train, family=binomial)
summary(framinghamLog)
sets typically between 50% and 80% in the training set
set.seed(1000)
split = sample.split(framingham$TenYearCHD, SplitRatio = 0.65)
# Split up the data using subset
train = subset(framingham, split==TRUE)
test = subset(framingham, split==FALSE)
# Logistic Regression Model
framinghamLog = glm(TenYearCHD ~ ., data = train, family=binomial)
summary(framinghamLog)
predictTest = predict(framinghamLog, type="response", newdata=test)
table(test$TenYearCHD, predictTest > 0.5)
(1069+11)/(1069+6+187+11)
(1069+6)/(1069+6+187+11)
# Test set AUC
library(ROCR)
ROCRpred = prediction(predictTest, test$TenYearCHD)
as.numeric(performance(ROCRpred, "auc")@y.values)
library(ROCR)
PredictROC = predict(StevensTree, newdata = Test)
PredictROC
PredictCART = predict(StevensTree, newdata = Test, type = "class")
table(Test$Reverse, PredictCART)
(41+71)/(41+36+22+71)
# ROC curve
library(ROCR)
PredictROC = predict(StevensTree, newdata = Test)
StevensTree = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, method="class", data = Train, control=rpart.control(minbucket=25))
# plot our tree using the prp function
# CRI is short for Criminal Defendant, INJ is short for Injured Person, etc...
prp(StevensTree)
# Make predictions
PredictCART = predict(StevensTree, newdata = Test, type = "class")
table(Test$Reverse, PredictCART)
(41+71)/(41+36+22+71)
# ROC curve
library(ROCR)
PredictROC = predict(StevensTree, newdata = Test)
StevensTree = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, method="class", data = Train, control=rpart.control(minbucket=25))
library(rpart)
# install.packages("rpart.plot")
stevens = read.csv("F:/SkyDrive/Studying/MIT_COURSES/15-071-TheAnalyticsEdge/lecture/dataset/stevens.csv")
# 1. Docket: just a unique identifier for each case
# 2. Term: the year of the case
# 3. six independent variables:
#       the circuit court of origin
#	the issue area of the case
#	the type of petitioner
#	the type of respondent
#	the lower court direction
#	whether or not the petitioner argued that a law or practice was unconstitutional
# 4. whether or not Justice Stevens voted to reverse the case: 1 for reverse and 0 for affirm
str(stevens)
# Split the data
library(caTools)
set.seed(3000)
split = sample.split(stevens$Reverse, SplitRatio = 0.7)
Train = subset(stevens, split==TRUE)
Test = subset(stevens, split==FALSE)
# Install rpart library that contains CART
# install.packages("rpart")
library(rpart)
# install.packages("rpart.plot")
library(rpart.plot)
# CART model
StevensTree = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, method="class", data = Train, control=rpart.control(minbucket=25))
# plot our tree using the prp function
# CRI is short for Criminal Defendant, INJ is short for Injured Person, etc...
prp(StevensTree)
# Make predictions
PredictCART = predict(StevensTree, newdata = Test, type = "class")
table(Test$Reverse, PredictCART)
(41+71)/(41+36+22+71)
# ROC curve
library(ROCR)
PredictROC = predict(StevensTree, newdata = Test)
PredictROC
PredictROC = predict(StevensTree, newdata = Test)
PredictROC
pred = prediction(PredictROC[,2], Test$Reverse)
perf = performance(pred, "tpr", "fpr")
plot(perf)
install.packages("randomForest")
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, ntree=200, nodesize=25 )
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, ntree=200, nodesize=25 )
library(randomForest)
library(randomForest)
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, ntree=200, nodesize=25 )
Train$Reverse = as.factor(Train$Reverse)
Test$Reverse = as.factor(Test$Reverse)
# Try again
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, ntree=200, nodesize=25 )
# Make predictions
PredictForest = predict(StevensForest, newdata = Test)
table(Test$Reverse, PredictForest)
(40+74)/(40+37+19+74)
install.packages("caret")
install.packages("e1071")
library(caret)
library(e1071)
fitControl = trainControl( method = "cv", number = 10 )
# use cp values from 0.01 through 0.5
cartGrid = expand.grid( .cp = (1:50)*0.01)
# Perform the cross validation
# Validate our parameters for our CART tree
train(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method = "rpart", trControl = fitControl, tuneGrid = cartGrid )
# Create a new CART model
StevensTreeCV = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, method="class", data = Train, control=rpart.control(cp = 0.18))
# Make predictions
# Select the optimal model using the largest value
StevensTreeCV = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, method="class", data = Train, control=rpart.control(cp = 0.18))
# Make predictions
PredictCV = predict(StevensTreeCV, newdata = Test, type = "class")
table(Test$Reverse, PredictCV)
(59+64)/(59+18+29+64)
hist(USDA$VitaminC, xlab = "Vitamin C (mg)", main = "Histogram of Vitamin C", xlim = c(0,100))
setwd("F:/SkyDrive/Studying/Stanford/introductionToStatisticalLearning/Rsession")
library(MASS)
library(ISLR)
